{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "MUVU5gazGgCz",
        "outputId": "4f802b06-553e-4c2a-f072-db2a4e83f47b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Invalid action",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 190\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m    189\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action(state)\n\u001b[1;32m--> 190\u001b[0m     next_state, reward, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    191\u001b[0m     agent\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mstore(state, action, next_state, reward, done)\n\u001b[0;32m    192\u001b[0m     agent\u001b[39m.\u001b[39mlearn(batch_size)\n",
            "Cell \u001b[1;32mIn[6], line 29\u001b[0m, in \u001b[0;36mCustomEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     28\u001b[0m     \u001b[39mif\u001b[39;00m action \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m action \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space_size:\n\u001b[1;32m---> 29\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid action\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m     \u001b[39m# Define the transition dynamics and rewards based on your problem\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[39m# For demonstration purposes, let's assume a simple environment where the goal is to reach state 0\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m action \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "\u001b[1;31mValueError\u001b[0m: Invalid action"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import time\n",
        "import csv\n",
        "from itertools import product\n",
        "import math\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class CustomEnvironment:\n",
        "    def __init__(self, state_space_size, action_space_size):\n",
        "        self.state_space_size = state_space_size\n",
        "        self.action_space_size = action_space_size\n",
        "        self.state = None\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.random.randint(0, self.state_space_size)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        if action < 0 or action >= self.action_space_size:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "\n",
        "        # Define the transition dynamics and rewards based on your problem\n",
        "        # For demonstration purposes, let's assume a simple environment where the goal is to reach state 0\n",
        "        if self.state == 0 and action == 0:\n",
        "            next_state = self.state\n",
        "            reward = 1  # Positive reward for reaching the goal\n",
        "            done = True  # Episode terminates\n",
        "        elif self.state == 0 and action != 0:\n",
        "            next_state = self.state\n",
        "            reward = -0.1  # Negative reward for taking an action other than the goal\n",
        "            done = False\n",
        "        else:\n",
        "            next_state = np.random.randint(0, self.state_space_size)\n",
        "            reward = 0\n",
        "            done = False\n",
        "\n",
        "        self.state = next_state\n",
        "        return next_state, reward, done\n",
        "\n",
        "\n",
        "\n",
        "class DQNNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, lr=1e-3):\n",
        "        super(DQNNet, self).__init__()\n",
        "        self.dense1 = nn.Linear(input_size, 400)\n",
        "        self.dense2 = nn.Linear(400, 300)\n",
        "        self.dense3 = nn.Linear(300, output_size)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = F.relu(self.dense2(x))\n",
        "        x = self.dense3(x)\n",
        "        return x\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        torch.save(self.state_dict(), filename)\n",
        "\n",
        "    def load_model(self, filename, device):\n",
        "        self.load_state_dict(torch.load(filename, map_location=device))\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer_state = []\n",
        "        self.buffer_action = []\n",
        "        self.buffer_next_state = []\n",
        "        self.buffer_reward = []\n",
        "        self.buffer_done = []\n",
        "        self.idx = 0\n",
        "\n",
        "    def store(self, state, action, next_state, reward, done):\n",
        "        if len(self.buffer_state) < self.capacity:\n",
        "            self.buffer_state.append(state)\n",
        "            self.buffer_action.append(action)\n",
        "            self.buffer_next_state.append(next_state)\n",
        "            self.buffer_reward.append(reward)\n",
        "            self.buffer_done.append(done)\n",
        "        else:\n",
        "            self.buffer_state[self.idx] = state\n",
        "            self.buffer_action[self.idx] = action\n",
        "            self.buffer_next_state[self.idx] = next_state\n",
        "            self.buffer_reward[self.idx] = reward\n",
        "            self.buffer_done[self.idx] = done\n",
        "        self.idx = (self.idx + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, device):\n",
        "        indices_to_sample = random.sample(range(len(self.buffer_state)), batch_size)\n",
        "\n",
        "        states = torch.tensor(self.buffer_state)[indices_to_sample].float().to(device)\n",
        "        actions = torch.tensor(self.buffer_action)[indices_to_sample].to(device)\n",
        "        next_states = torch.tensor(self.buffer_next_state)[indices_to_sample].float().to(device)\n",
        "        rewards = torch.tensor(self.buffer_reward)[indices_to_sample].float().to(device)\n",
        "        dones = torch.tensor(self.buffer_done)[indices_to_sample].to(device)\n",
        "\n",
        "        return states, actions, next_states, rewards, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer_state)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, device, state_size, action_size,\n",
        "                 discount=0.99,\n",
        "                 eps_max=1.0,\n",
        "                 eps_min=0.01,\n",
        "                 eps_decay=0.995,\n",
        "                 memory_capacity=5000,\n",
        "                 lr=1e-3,\n",
        "                 train_mode=True):\n",
        "\n",
        "        self.device = device\n",
        "        self.epsilon = eps_max\n",
        "        self.epsilon_min = eps_min\n",
        "        self.epsilon_decay = eps_decay\n",
        "        self.discount = discount\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.policy_net = DQNNet(self.state_size, self.action_size, lr).to(self.device)\n",
        "        self.target_net = DQNNet(self.state_size, self.action_size, lr).to(self.device)\n",
        "        self.target_net.eval()\n",
        "        if not train_mode:\n",
        "            self.policy_net.eval()\n",
        "        self.memory = ReplayMemory(capacity=memory_capacity)\n",
        "\n",
        "    def update_target_net(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if random.random() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        if not torch.is_tensor(state):\n",
        "            state = torch.tensor([state], dtype=torch.float32).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            action = self.policy_net.forward(state)\n",
        "        return torch.argmax(action).item()\n",
        "\n",
        "    def learn(self, batch_size):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "        states, actions, next_states, rewards, dones = self.memory.sample(batch_size, self.device)\n",
        "        q_pred = self.policy_net.forward(states).gather(1, actions.view(-1, 1))\n",
        "        q_target = self.target_net.forward(next_states).max(dim=1).values\n",
        "        q_target[dones] = 0.0\n",
        "        y_j = rewards + (self.discount * q_target)\n",
        "        y_j = y_j.view(-1, 1)\n",
        "        self.policy_net.optimizer.zero_grad()\n",
        "        loss = F.mse_loss(y_j, q_pred).mean()\n",
        "        loss.backward()\n",
        "        self.policy_net.optimizer.step()\n",
        "\n",
        "    def save_model(self, filename):\n",
        "        self.policy_net.save_model(filename)\n",
        "\n",
        "    def load_model(self, filename):\n",
        "        self.policy_net.load_model(filename=filename, device=self.device)\n",
        "\n",
        "# Define your other utility functions here\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize your DQNAgent\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    state_size = 10  # Example state size\n",
        "    action_size = 4  # Example action size\n",
        "    agent = DQNAgent(device, state_size, action_size)\n",
        "    num_episodes = 10\n",
        "    batch_size=10\n",
        "    # Example usage\n",
        "    env = CustomEnvironment(state_space_size=5, action_space_size=2)\n",
        "    # Training loop\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.select_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            agent.memory.store(state, action, next_state, reward, done)\n",
        "            agent.learn(batch_size)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        agent.update_epsilon()\n",
        "        agent.update_target_net()\n",
        "\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
